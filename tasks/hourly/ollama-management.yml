---
# Ollama AI model service configuration and management tasks
# These tasks run hourly to ensure Ollama is properly configured with GPU optimization

- name: "Create Ollama directory structure on fast storage"
  file:
    path: "/opt/storage/lab-fast/projects/hosted-services/ollama"
    state: directory
    owner: root
    group: root
    mode: '0755'

- name: "Download Ollama installation script" 
  get_url:
    url: "https://ollama.ai/install.sh"
    dest: "/tmp/ollama-install.sh"
    mode: '0755'
    force: yes
  register: ollama_script_download

- name: "Check if Ollama is already installed"
  stat:
    path: "/usr/local/bin/ollama"
  register: ollama_binary_check

- name: "Check if Ollama has CUDA support"
  shell: "ldd /usr/local/bin/ollama | grep -E '(cuda|nv)' | wc -l"
  register: ollama_cuda_check
  when: ollama_binary_check.stat.exists
  ignore_errors: yes

- name: "Install/Reinstall Ollama with CUDA support"
  shell: "curl -fsSL https://ollama.ai/install.sh | bash"
  when: not ollama_binary_check.stat.exists or ollama_script_download.changed or (ollama_cuda_check is defined and ollama_cuda_check.stdout == "0")
  register: ollama_install_result
  ignore_errors: yes

- name: "Create ollama user with custom home directory"
  user:
    name: ollama
    system: yes
    shell: /bin/false
    home: /opt/storage/lab-fast/projects/hosted-services/ollama
    create_home: yes
    groups: 
      - render
      - video
    append: yes
  ignore_errors: yes

- name: "Set ownership for Ollama directory"
  file:
    path: "/opt/storage/lab-fast/projects/hosted-services/ollama"
    owner: ollama
    group: ollama
    recurse: yes
    state: directory

- name: "Update Ollama systemd service with optimized AI performance configuration"
  copy:
    content: |
      [Unit]
      Description=Ollama Service
      After=network-online.target

      [Service]
      ExecStart=/usr/local/bin/ollama serve
      User=ollama
      Group=ollama
      Restart=always
      RestartSec=3
      Environment="PATH=/usr/local/bin:/usr/bin:/bin:/usr/local/cuda/bin"
      Environment="LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu:/usr/local/cuda/lib64"
      Environment="OLLAMA_HOME=/opt/storage/lab-fast/projects/hosted-services/ollama"
      Environment="OLLAMA_HOST=0.0.0.0:11434"
      Environment="CUDA_VISIBLE_DEVICES=0"
      Environment="OLLAMA_GPU_LAYERS=-1"
      Environment="OLLAMA_NUM_GPU=1"
      Environment="OLLAMA_GPU_OVERHEAD=0"
      Environment="OLLAMA_MAX_LOADED_MODELS=1"
      Environment="OLLAMA_KEEP_ALIVE=30m"
      Environment="OLLAMA_PARALLEL_REQUESTS=1"
      Environment="OLLAMA_FLASH_ATTENTION=1"
      Environment="OLLAMA_BATCH_SIZE=2048"
      Environment="OLLAMA_CTX_SIZE=8192"
      Environment="OLLAMA_GPU_TARGET=cuda"
      Environment="OLLAMA_LLM_LIBRARY=cuda"
      Environment="OLLAMA_FORCE_OFFLOAD=1"
      Environment="OLLAMA_DISABLE_CPU_FALLBACK=1"
      Environment="OLLAMA_VRAM_FRACTION=0.95"
      Environment="CUDA_DEVICE_ORDER=PCI_BUS_ID"
      Environment="CUDA_FORCE_PTX_JIT=1"
      Environment="CUDA_CACHE_DISABLE=0"
      Environment="CUDA_LAUNCH_BLOCKING=0"
      Environment="CUDA_MALLOC_SYNC=0"
      Environment="OLLAMA_DEBUG=0"

      [Install]
      WantedBy=default.target
    dest: "/etc/systemd/system/ollama.service"
    owner: root
    group: root
    mode: '0644'
  register: ollama_service_changed

- name: "Reload systemd and enable Ollama service"
  systemd:
    name: ollama
    daemon_reload: yes
    enabled: yes
    state: started
  when: ollama_binary_check.stat.exists or ollama_install_result is succeeded

- name: "Configure GPU for maximum performance"
  shell: |
    nvidia-smi -pm 1 || true
    nvidia-smi -pl 170 || true
  ignore_errors: yes

- name: "Restart Ollama service if configuration changed"
  systemd:
    name: ollama
    state: restarted
  when: ollama_service_changed.changed

- name: "Verify Ollama service is running"
  wait_for:
    port: 11434
    host: localhost
    timeout: 30
  register: ollama_accessibility
  ignore_errors: yes

- name: "Verify GPU acceleration is working"
  shell: |
    # Check if Ollama detects GPU
    journalctl -u ollama -n 20 | grep -E "(NVIDIA|cuda|GPU)" | tail -1
  register: ollama_gpu_detection
  ignore_errors: yes
  when: ollama_accessibility is succeeded

- name: "Test GPU inference performance (quick validation)"
  shell: |
    # Quick test with timeout - check if GPU memory increases during inference
    timeout 10s sh -c '
      nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits > /tmp/gpu_before.txt &
      sleep 2
      echo "test" | ollama run tinyllama:latest --verbose > /tmp/ollama_test.log 2>&1 &
      sleep 6
      nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits > /tmp/gpu_after.txt
      wait
    ' || true
    
    # Compare memory usage
    BEFORE=$(cat /tmp/gpu_before.txt 2>/dev/null || echo "0")  
    AFTER=$(cat /tmp/gpu_after.txt 2>/dev/null || echo "0")
    if [ "$AFTER" -gt "$((BEFORE + 100))" ]; then
      echo "GPU acceleration confirmed: ${BEFORE}MB -> ${AFTER}MB"
    else
      echo "GPU acceleration not detected: ${BEFORE}MB -> ${AFTER}MB"
    fi
  register: ollama_gpu_performance
  ignore_errors: yes
  when: ollama_accessibility is succeeded

- name: "Log Ollama configuration status"
  lineinfile:
    path: "{{ validation_log }}"
    line: "{{ ansible_date_time.iso8601 }} - Ollama configuration {{ 'verified with GPU' if (ollama_accessibility is succeeded and ollama_gpu_detection is succeeded) else ('verified CPU-only' if ollama_accessibility is succeeded else 'failed') }} - {{ ollama_gpu_performance.stdout | default('no performance test') }}"